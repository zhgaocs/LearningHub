# 全连接神经网络

## 1. 全连接神经网络概述

1. 定义与组成：全连接神经网络是一种基础的神经网络结构，它由多个层组成，每一层都与前一层和后一层的所有神经元相连

2. 全连接神经网络主要由三部分组成：输入层、隐层和输出层（层数统计时**不包括**输入层）

    + **输入层**：输入层接收原始数据，将原始数据转化为可以被神经网络处理的形式

    + **隐层**：隐层是神经网络的核心，每一层都包含了一定数量的神经元。每个神经元都会接收前一层神经元的输出，通过激活函数处理后，将结果传递给下一层神经元

    + **输出层**：输出层是神经网络的最后一层，它的任务是将隐层的处理结果转化为需要的形式，例如分类任务中的类别标签，回归任务中的实数值等

## 2. 激活函数

### 2.1 激活函数的作用

全连接神经网络缺少激活函数会退化为线性分类器

### 2.2 常用的激活函数

1. Sigmoid

$$
f(x) = \frac{1}{1+e^{-x}}
$$

2. ReLU

$$
f(x) = \max(0, x)
$$

3. Leaky ReLU（例如 $a = 0.01$）

$$
f(x) = \max(ax, x)
$$

4. Tanh

$$
f(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}
$$

### 2.3 激活函数的选择

+ **梯度消失**：在深度神经网络的训练过程中，随着网络层数的增加，梯度在反向传播过程中逐渐变得非常小，最终可能接近于零。这会导致前面的层几乎没有更新，使得模型难以学习或训练速度极慢

+ **梯度爆炸**：在深度神经网络的训练过程中，随着网络层数的增加，梯度在反向传播过程中不断变大，最终可能达到非常大的值。这会导致模型的权重更新过大，导致训练不稳定甚至发散

---

使用ReLU激活函数，ReLU激活函数在正值部分的导数为1，能够有效减缓梯度消失问题

## 3. 梯度下降算法改进

### 3.1 梯度下降法存在的问题

+ **局部极小值和鞍点**：陷入局部极小值或鞍点（梯度为零但不是极值点），无法继续优化
+ **震荡问题**：在损失函数的某些方向上变化迅速，而在另一些方向上变化缓慢，导致算法震荡，行进缓慢
+ **收敛速度慢**：对于一些函数，梯度下降的收敛速度可能非常慢，尤其是在梯度很小的区域。此外，当学习率设置不当时，算法可能需要非常多的迭代才能收敛

### 3.2 动量法

**思想**：利用累加历史梯度信息更新梯度

更新公式：

$$
\begin{aligned}
v_t &= \gamma v_{t-1} + \eta \nabla_\theta L(\theta_t) \\
\theta_{t+1} &= \theta_t - v_t
\end{aligned}
$$

其中：
+ $\theta$ 为模型参数
+ $t$ 为迭代次数
+ $\eta$ 为学习率
+ $L(\theta)$ 为损失函数
+ $\nabla_\theta L(\theta)$ 为损失函数关于参数的梯度
+ $v$ 为动量
+ $\gamma$ 为动量因子（例如设为0.9）


### 3.3 RMSProp

更新公式：

$$
\begin{aligned}
g_t &= \nabla_\theta L(\theta_t) \\
r_t &= \beta r_{t-1} + (1-\beta) g_t^2 \\
\theta_{t+1} &= \theta_t - \frac{\eta}{\sqrt{r_t + \epsilon}} g_t
\end{aligned}
$$

其中：
+ $g$ 为梯度
+ $r_t$ 为梯度平方的指数加权平均
+ $\beta$ 表示衰减率（例如设为0.9）
+ $\epsilon$ 是一个很小的常数，用于防止除零错误（例如设为1e-8）

### 3.4 Adam

更新公式：

$$
\begin{aligned}
g_t &= \nabla_\theta L(\theta_t) \\
m_t &= \beta_1 m_{t-1} + (1-\beta_1) g_t \\
v_t &= \beta_2 v_{t-1} + (1-\beta_2) g_t^2 \\
\hat{m_t} &= \frac{m_t}{1-\beta_1^t} \\
\hat{v_t} &= \frac{v_t}{1-\beta_2^t} \\
\theta_{t+1} &= \theta_t - \frac{\eta}{\sqrt{\hat{v_t}} + \epsilon} \hat{m_t}
\end{aligned}
$$

其中：
+ $m_t$ 为梯度的一阶矩估计
+ $v_t$ 为梯度的二阶矩估计
+ $\beta_1$ 和 $\beta_2$ 为动量项和RMS项的衰减率（例如设为0.9和0.999）
+ $\hat{m_t}$ 和 $\hat{v_t}$ 为 $m_t$ 和 $v_t$ 的偏差修正

## 4. 权值初始化

### 4.1 初始化

1. 全零初始化：会导致网络中不同的神经元有相同的输出，进行同样的参数更新，等价于一个神经元。因此，建议采用随机初始化，避免全零初始化

2. 有效的初始化方法是使网络各层的激活值和局部梯度的方差在传播过程中保持一致，以保持网络正向和反向数据流动

### 4.2 Xavier初始化

+ 均匀分布

$$
W \sim \mathcal{U}(-\sqrt{\frac{6}{n_{in}+n_{out}}}, \sqrt{\frac{6}{n_{in}+n_{out}}})
$$

+ 正态分布

$$
W \sim \mathcal{N}(0, \frac{2}{n_{in}+n_{out}})
$$

其中： $n_{in}$是输入神经元的数量， $n_{out}$是输出神经元的数量

### 4.3 He初始化

$$
W \sim \mathcal{N}(0, \frac{2}{n})
$$

其中： $n$ 是输入层神经元的个数

### 4.4 与激活函数的搭配

+ 激活函数为Tanh或Sigmoid时，使用Xavier初始化
+ 激活函数为ReLU或Leakly ReLU时，使用He初始化

## 5. 交叉熵损失

### 5.1 熵

熵是度量一个概率分布的平均不确定性的度量。对于离散随机变量 $X$ 及其概率分布 $P$，熵的定义为：

$$
H(X) = -\sum_i P(x_i) \log P(x_i)
$$

其中： $P(X_i)$ 是随机变量 $X$ 取值为 $x_i$ 的概率

### 5.2 交叉熵：

交叉熵度量两个概率分布 $P$ 和 $Q$ 之间的不相似性。它定义为在概率分布 $P$ 下，用概率分布 $Q$ 编码数据时的平均代价。公式为：

$$
H(P, Q) = - \sum_i P(x_i) \log Q(x_i)
$$

其中：

+ $P(x_i)$ 是真实概率分布
+ $Q(x_i)$ 是预测概率分布

### 5.3 相对熵：

相对熵（或KL散度）度量两个概率分布 $P$ 和 $Q$ 之间的差异。它表示在分布 $P$ 下，使用分布 $Q$ 进行编码时的额外代价。公式为：

$$
D_{KL}(P||Q) = -\sum_i P(x_i) \log \frac{Q(x_i)}{P(x_i)} = \sum_i P(x_i) \log \frac{P(x_i)}{Q(x_i)}
$$

+ $P(x_i)$ 是真实概率分布
+ $Q(x_i)$ 是预测概率分布

## 6. 批归一化

### 6.1 定义

标准化输入数据，使得每个特征的分布均值为0，标准差为1，使得神经网络在训练过程中更稳定，更易训练

### 6.2 算法

+ 输入： $B = \{x_1, x_2, \cdots, x_m\}$
+ 输出： $\{y_1, y_2, \cdots, y_m\}$
+ 学习参数： $\gamma , \beta$

$$
y_i = BN(x_i) = \gamma \frac{x_i - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}} + \beta
$$

### 6.3 批归一化的位置

通常位于神经网络的每一层（特别是卷积层或全连接层）之后，激活函数之前

## 7. 拟合

### 7.1 欠拟合与过拟合

+ **欠拟合**：模型过于简单，无法很好地学习数据中的规律，导致对已知数据和未知数据的预测都不准确
+ **过拟合**：模型过于复杂，包含了过多的参数，导致模型在已知数据上的预测很好，但在未知数据上的预测很差。这种情况下，模型可能只是记住了训练集数据，而不是学习到了数据特征

### 7.2 随机失活（Dropout）

1. **定义**：在**训练**中以一定概率使神经元不被激活，这意味着“失活”的神经元暂时不参与前向传播和反向传播
2. **实现方式**：将失活的神经元输出值设置为0
3. **防止过拟合**：通过随机失活部分神经元，防止模型过于依赖某些特定的神经元或特征，从而提高泛化能力

## 8. 超参数优化

1. **网格搜索**：

    + **方法**：将每个超参数的多个取值组合成多个参数集，在验证集上评估每个参数集的性能，选择表现最好的参数集
    + **优点**：全面搜索所有可能的参数组合，确保找到最优解
    + **缺点**：计算成本高，尤其是参数空间较大时

2. **随机搜索**：

    + **方法**：在参数空间内随机采样多个点，在验证集上评估每个参数集的性能，选择表现最好的参数集
    + **优点**：计算成本较低，适用于大参数空间
    + **缺点**：可能会错过最优解，但通常能找到接近最优的解

3. **搜索策略**

    + **粗搜索**：在较大范围内采样超参数，找到初步的最佳范围
    + **细搜索**：在初步范围内进行更细致的采样，进一步优化超参数

4. 对数空间采样：对于学习率、正则化强度等超参数，在对数空间内进行随机采样，以获得更好的结果
