# 线性分类器

## 1. 图像表示

+ 二进制图像：二维矩阵，每个元素值为0或1
+ 灰度图像：二维矩阵，每个元素值为0-255
+ 彩色图像：RGB模型、CMY/CMYK模型、HSI模型

---

图像的一种向量化表示：

$$
x = 
\begin{bmatrix}
r_1 \\
g_1 \\
b_1 \\
\vdots \\
r_n \\
g _n \\
b_n 
\end{bmatrix}
$$

## 2. 线性分类器

通过特征的线性组合做出分类决定，如果输入的特征向量是实数向量 $x$ ，则输出的分数为：

$$
y = f(Wx) = f(\sum_j w_jx_j)
$$

其中 $W$ 是一个权重向量， $f$ 是一个函数

## 3. 损失函数

### 3.1 定义

用于衡量模型的预测值与真实值之间的差异。它是一个非负实值函数，通常表示为 $L(Y, \hat{Y})$ ，其中 $Y$ 是真实值， $\hat{Y}$ 是模型的预测值

### 3.2 Hinge损失

$$
L_i = \sum_{j \neq y_i} 
\begin{cases}
0 & \text{if } s_{y_i} \geq s_{ij} + 1 \\
s_{ij} - s_{y_i} + 1 & \text{otherwise}
\end{cases}
= \sum_{j \neq y_i} \max(0, s_{ij} - s_{y_i} + 1)
$$

其中：

+ $j$：类别标签
+ $s_{ij}$：第 $i$ 个样本第 $j$ 类别的预测分数
+ $s_{y_i}$：第 $i$ 个样本真实类别的预测分数

### 3.3 正则项

正则项用于防止模型过拟合。它通过在损失函数中添加一个惩罚项来限制模型的复杂度，从而提高模型的泛化能力

+ L1正则化：

$$
R(W) = \sum_k \sum_l |W_{k,l}|
$$

+ L2正则化：鼓励使用所有特征，而不是依赖少数几维特征

$$
R(W) = \sum_k \sum_l W_{k,l}^2
$$

+ Elastic Net正则化：结合L1和L2

$$
R(W) = \sum_k \sum_l \beta W_{k,l}^2 + |W_{k,l}|
$$

### 3.4 参数优化

利用损失函数 $L(W)$ 的输出值作为反馈信号来调整参数，提升模型预测性能

---

**梯度下降法**：沿着损失函数梯度的反方向更新模型参数，更新公式为：

$$
W = W - \eta \nabla L(W)
$$

其中： $\eta$ 是学习率， $\nabla L(W)$ 是损失函数 $L(W)$ 对参数 $W$ 的梯度

梯度下降法的类型：
  + 随机梯度下降法：每次迭代只使用一个样本来计算梯度并更新参数
  + 小批量梯度下降法：每次迭代使用一小批样本来计算梯度并更新参数

## 4. 超参数

### 4.1 定义

在开始学习之前设置的参数，而不是通过训练得到的参数

### 4.2 常见的超参数

+ **学习率**：模型参数更新的步长
+ **批量大小**：每次更新参数时使用的样本数量
+ **正则化参数**
+ **隐藏层的神经元数量**

## 5. 数据处理

### 5.1 数据预处理

1. **去均值**：从每个特征中减去其平均值
2. **归一化**：将数据缩放到相同的范围，例如 $[0,1]$ 或 $[-1,1]$ 
3. **去相关**：通过某种线性变换，使数据中的各个特征之间不再相关（即协方差为零）
4. **白化**：将数据变换为均值为零、协方差矩阵为单位矩阵（即特征之间不相关且具有相同的方差）

### 5.2 数据集划分

+ **训练集**：用于学习参数的数据集
+ **验证集**：用于选择超参数的数据集
+ **测试集**：用于评估模型泛化能力的数据集

### 5.3 K折交叉验证

将数据集分成K个大小相等的子集，每次使用其中一个子集作为验证集，其余作为训练集（适用于数据有限的情况）

+ 第一次：使用第1折作为测试集，剩余的K-1折作为训练集
+ 第二次：使用第2折作为测试集，剩余的K-1折作为训练集
+ ...
+ 第K次：使用第K折作为测试集，剩余的K-1折作为训练集
