{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def softmax_loss_naive(W, X, y, reg):\n",
    "    \"\"\"\n",
    "    Softmax loss function, naive implementation (with loops)\n",
    "\n",
    "    Inputs have dimension D, there are C classes, and we operate on minibatches\n",
    "    of N examples.\n",
    "\n",
    "    Inputs:\n",
    "    - W: A numpy array of shape (D, C) containing weights.\n",
    "    - X: A numpy array of shape (N, D) containing a minibatch of data.\n",
    "    - y: A numpy array of shape (N,) containing training labels; y[i] = c means\n",
    "      that X[i] has label c, where 0 <= c < C.\n",
    "    - reg: (float) regularization strength\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - loss as single float\n",
    "    - gradient with respect to weights W; an array of same shape as W\n",
    "    \"\"\"\n",
    "    # Initialize the loss and gradient to zero.\n",
    "    loss = 0.0\n",
    "    dW = np.zeros_like(W)\n",
    "    num_classes = W.shape[1]\n",
    "    num_train = X.shape[0]\n",
    "\n",
    "    for i in range(num_train):\n",
    "        scores = X[i].dot(W)\n",
    "        scores -= np.max(scores)  # Numeric stability fix\n",
    "        sum_exp_scores = np.sum(np.exp(scores))\n",
    "        correct_class_score = scores[y[i]]\n",
    "        loss += -correct_class_score + np.log(sum_exp_scores)\n",
    "\n",
    "        for j in range(num_classes):\n",
    "            softmax_output = np.exp(scores[j]) / sum_exp_scores\n",
    "            if j == y[i]:\n",
    "                dW[:, j] += (-1 + softmax_output) * X[i]\n",
    "            else:\n",
    "                dW[:, j] += softmax_output * X[i]\n",
    "\n",
    "    loss /= num_train\n",
    "    dW /= num_train\n",
    "\n",
    "    # Add regularization to the loss and gradient\n",
    "    loss += 0.5 * reg * np.sum(W * W)\n",
    "    dW += reg * W\n",
    "\n",
    "    return loss, dW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "某样本 $i$ 在类别 $j$ 上的得分：\n",
    "\n",
    "$$\n",
    "s_j = X_i W_{:,j} \\\\\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "softmax函数\n",
    "\n",
    "$$\n",
    "\\text{softmax}(s_j) = \\frac{e^{s_j}}{\\sum_{k=1}^C e^{s_k}}\n",
    "$$\n",
    "\n",
    "其中： $C$ 是类别数\n",
    "\n",
    "---\n",
    "\n",
    "损失函数\n",
    "\n",
    "1. 交叉熵损失：\n",
    "\n",
    "$$\n",
    "L_i = -\\log(\\frac{e^{s_{y_i}}}{\\sum_{k=1}^C e^{s_k}})\n",
    "= -s_{y_i} + \\log(\\sum_{k=1}^C e^{s_k})\n",
    "$$\n",
    "\n",
    "\n",
    "其中： $s_{y_i}$ 是样本 $i$ 在正确类别 $y_i$ 上的分数\n",
    "\n",
    "\n",
    "2. 总损失：\n",
    "\n",
    "$$\n",
    "L = \\frac{1}{N} \\sum_{i=1}^N L_i + \\frac{\\lambda}{2} \\sum_j \\sum_{k=1}^C W_{j,k}^2\n",
    "$$\n",
    "\n",
    "其中：\n",
    "\n",
    "+ $N$ 是样本数\n",
    "+ $\\frac{\\lambda}{2} \\sum_j \\sum_k W_{j,k}^2$ 是正则项， $\\lambda$ 是正则化强度\n",
    "\n",
    "---\n",
    "\n",
    "对 $L_i$ 求导\n",
    "\n",
    "1. 对 $-s_{y_i}$ 求导：\n",
    "\n",
    "$$\n",
    "\\frac{\\partial (-s_{y_i})}{\\partial W_{:,j}} = \\frac{\\partial (-X_i W_{:,y_i})}{\\partial W_{:,j}}\n",
    "$$\n",
    "\n",
    "+ 当 $j = y_i$ 时：\n",
    "\n",
    "$$\n",
    "\\frac{\\partial (-X_i W_{:,y_i})}{\\partial W_{:,j}} = \\frac{\\partial (-X_i W_{:,y_i})}{\\partial W_{:,y_i}} = -X_i\n",
    "$$\n",
    "\n",
    "+ 当 $j \\neq y_i$ 时：\n",
    "\n",
    "$$\n",
    "\\frac{\\partial (-X_i W_{:,y_i})}{\\partial W_{:,j}} = 0\n",
    "$$\n",
    "\n",
    "\n",
    "2. 对 $\\log(\\sum_{k=1}^C e^{s_k})$ 求导\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial \\log(\\sum_{k=1}^C e^{s_k})}{\\partial W_{:,j}} &= \\frac{1}{\\sum_{k=1}^C e^{s_k}} \\cdot \\frac{\\partial (\\sum_{k=1}^C e^{s_k})}{\\partial W_{:,j}} \\\\ &= \\frac{1}{\\sum_{k=1}^C e^{s_k}} \\cdot \\frac{\\partial e^{s_j}}{\\partial W_{:,j}} \\\\ &= \\frac{e^{s_j}}{\\sum_{k=1}^C e^{s_k}} \\cdot \\frac{\\partial s_j}{\\partial W_{:,j}} \\\\ &= \\frac{e^{s_j}}{\\sum_{k=1}^C e^{s_k}} \\cdot \\frac{\\partial (X_i W_{:,j})}{\\partial W_{:,j}} \\\\ &= \\text{softmax}(s_j) X_i\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "3. 整理得：\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L_i}{\\partial W_{:,j}} = \n",
    "\\begin{cases}\n",
    "(\\text{softmax}(s_j) - 1) X_i& \\text{if } j = y_i \\\\\n",
    "\\text{softmax}(s_j) X_i & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "最后：\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W} = \\frac{1}{N} \\frac{\\partial L_i}{\\partial W_{:,j}} + \\lambda W\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs231n",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
